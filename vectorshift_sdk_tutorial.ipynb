{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f4b5f1",
   "metadata": {},
   "source": [
    "# Getting Started with Vectorshift SDK\n",
    "\n",
    "## Installing Library\n",
    "\n",
    "To use the VectorShift Python library, you should be using Python 3.10 or newer.\n",
    "\n",
    "The SDK is built upon our API. To access much of the functionality, such as saving and downloading pipelines, you should already have an API key ready.\n",
    "\n",
    "Our Python SDK is available as the vectorshift package on PyPl. Before downloading, ensure you have pip installed. Then, you can simply get started by downloading the package by running the command in your terminal of choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e3fee3-f095-4ed6-831d-2315d8a0331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vectorshift in /opt/anaconda3/lib/python3.11/site-packages (0.0.59)\r\n",
      "Requirement already satisfied: networkx==3.1 in /opt/anaconda3/lib/python3.11/site-packages (from vectorshift) (3.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install vectorshift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6bb89",
   "metadata": {},
   "source": [
    "## Tutorial: Personalized Email Generator\n",
    "\n",
    "Let's say we want to build and run a simple pipeline using the Python SDK. This walkthrough will give you a way to construct and view a pipeline, while also introducing the major building blocks of the SDK along the way. \n",
    "\n",
    "The ultimate pipeline we'll build is a copy of the Personalized Email Generator introduced as an example pipeline under the no-code documentation. As a brief overview, the pipeline takes in a text input of a company's website URL, queries a VectorDB to get information about the company, and passes the information into an LLM to generate a personalized email for outreach to the company (as a consulting firm looking to improve their operations).\n",
    "\n",
    "In the first step, lets import our SDK and put the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e6bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vectorshift\n",
    "from vectorshift.node import InputNode, URLLoaderNode, TextNode, VectorQueryNode, OpenAILLMNode, OutputNode\n",
    "from vectorshift.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0a686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_api_key=\"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7abcae",
   "metadata": {},
   "source": [
    "## Defining Pipeline Nodes\n",
    "\n",
    "VectorShift pipelines are created using nodes that represent units of computation and edges which define how outputs of nodes are fed as inputs to others. At a high-level, in the SDK we similarly create a Pipeline object by composing together nodes of different classes and parameters. There are several classes of nodes that correspond closely with the various nodes available in the no-code editor. Once initialized, each node object has one or more outputs, which we can pass into later nodes' constructors. \n",
    "\n",
    "In essence, most nodes will expect to take in the outputs of other nodes when initialized, which adds an edge in the computation graph between the nodes. So it makes the most sense to build the pipeline in order, starting with inputs and  sequentially feeding node outputs as inputs into later nodes.\n",
    "\n",
    "### Input\n",
    "Our pipeline takes in one input, which is of type text (the URL). Correspondingly, there's an InputNode class that we can use to represent this input, which requires a name and data type.\n",
    "\n",
    "The data type is more than a constructor argument here. Behind the scenes, node outputs are tagged with different types (e.g. LLMs produce textual output), which can help catch issues with pipelines before they're saved to the VectorShift platform. We list the expected types of different nodes' inputs and outputs in node-specific documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d55df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node = InputNode(name=\"input_1\", input_type=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e4449",
   "metadata": {},
   "source": [
    "Each node has one or more outputs with different names that can be fed as inputs into later nodes. This is represented for all node objects by a dictionary of names to output objects (NodeOutputs), given by the method outputs(). The majority of nodes, however, only have one output, in which case we can call the output() method directly to get the output object. \n",
    "\n",
    "There's a node class that can load the contents of a URL and return the data retrieved, which seems useful here. We'll take the output of input_node and feed it into the constructor, which expects a url_input argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e50b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: URL DataLoader node input url received input type text, which may be incompatible with expected type url\n"
     ]
    }
   ],
   "source": [
    "url_loader = URLLoaderNode(url_input=input_node.output())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741e332",
   "metadata": {},
   "source": [
    "The semantic meaning of the line above is that the output of the overall pipeline input gets fed as the input to the URL loader node. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588751e4",
   "metadata": {},
   "source": [
    "### Querying the VectorDB\n",
    "\n",
    "The output of url_loader can be used to query a VectorDB. As in the no-code walkthrough, we have a corresponding VectorQueryNode that takes one or more queries and one or more document inputs. This node will work by first loading the content given by the URL into a termporary VectorDB, and then using the string query to perform a semantic search over the VectorDB to retrieve the relevant contents of what we loaded from the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f531ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = TextNode(text=\"How can this company grow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898f208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query = VectorQueryNode(\n",
    "    query_input=[question_text.output()], \n",
    "    documents_input=[url_loader.output()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72457d31",
   "metadata": {},
   "source": [
    "Let's say we want to combine the query output with the question we used in the query. To do this, we can introduce another TextNode that includes the outputs of these two nodes as variables inside the node text. The functionality is the same as the no-code platform: each variable is indicated by double brackets {{}}, and we expect an input to be passed in corresponding to each variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b793427",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = TextNode(\n",
    "    text=\"Company Context: {{Context}}\\n Question: {{Question}}\",\n",
    "    text_inputs={\n",
    "        \"Context\": vector_query.output(),\n",
    "        \"Question\": question_text.output()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f769150",
   "metadata": {},
   "source": [
    "We introduced two variables, Context and Question, so we correspondingly pass in a text_inputs argument of previous nodes' outputs keyed by those variable names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67129a7a",
   "metadata": {},
   "source": [
    "### Generating Text with the LLM\n",
    "\n",
    "Let's use GPT-4 to generate a customized sentence for the email. In this case, we can use an OpenAILLMNode.  These nodes also support a system input prompt, which we can seed with some contextual text. (Note: the system text is shortened from the original tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f475832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_text_raw = \"\"\"You are a sentence generator for a consulting\n",
    "firm. You take in data from a website and generate a sentence\n",
    "that explains how the firm can help this company.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f328135",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_text = TextNode(text=system_text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fa24b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILLMNode(\n",
    "    model=\"gpt-4\", \n",
    "    system_input=system_text.output(), \n",
    "    prompt_input=prompt_text.output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74bae2",
   "metadata": {},
   "source": [
    "### Composing the Email\n",
    "\n",
    "The prompt for the LLM node was to generate a sentence, not an entire email. We can write up some custom text ourselves to fill in the rest of the email, and insert the generated sentence as a variable as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e929c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text_raw = \"\"\"Hello,\n",
    "We are XYZ consulting, specializing in crafting growth strategies.\n",
    "\n",
    "{{Personalized_Message}}\n",
    "\n",
    "Are you available anytime later this week to chat?\n",
    "\n",
    "Best,\n",
    "XYZ\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e223852",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = TextNode(\n",
    "    text=output_text_raw,\n",
    "    text_inputs={\n",
    "        \"Personalized_Message\": llm.output()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d500a",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The output of the entire pipeline should be the text of the email, which is created by the output_text node. We can just take that node's output() and package it in an OutputNode, which determines the overall returned value of the pipeline.\n",
    "\n",
    "Remember that OutputNode is a node that represents, in the pipeline's computation graph, the final value produced. We pass in the output() of output_text, which is a NodeOutput, as the input to that node. OutputNodes are a kind of node; NodeOutputs define what a node returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd53113",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = OutputNode(\n",
    "    name=\"output_1\", \n",
    "    output_type=\"text\", \n",
    "    input=output_text.output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2f038",
   "metadata": {},
   "source": [
    "These are all the nodes we need! The overall structure of the nodes closely follows that of the no-code example. Each node block in the no-code editor became its own object in Python, and each edge between nodes has been represented by the output of one node being passed into the constructor of another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416811ff",
   "metadata": {},
   "source": [
    "### Creating and Deploying the Pipeline\n",
    "\n",
    "Once nodes have been defined, creating a pipeline object is fairly simple, since the node objects themselves already encode the edges between them.\n",
    "\n",
    "A Pipeline object can be initialized by passing in a list of all nodes, a name, and a description. The list of nodes can be passed in in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b4e1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_gen_pipeline_nodes = [\n",
    "    input_node, url_loader, question_text, vector_query,\n",
    "    prompt_text, system_text, llm, output_text, output\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c606244",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_gen_pipeline = Pipeline(\n",
    "    name=\"Personalized Email Generator\",\n",
    "    description=\"Generate personalized emails for outreach\",\n",
    "    nodes=email_gen_pipeline_nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ed0c2",
   "metadata": {},
   "source": [
    "There are a few nifty methods that a Pipeline object has. Printing it gives a representation of its constituent nodesâ€”and if you want to generate code that represents how you could construct the object, there's a method for that too (that assigns generated IDs as variable names for each node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(email_gen_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(email_gen_pipeline.construction_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a5ac2",
   "metadata": {},
   "source": [
    "To save the pipeline to the VectorShift platform, we can either pass in our API keys to the save() method of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9e10dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = vectorshift.deploy.Config(\n",
    "    api_key=vs_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.save_new_pipeline(email_gen_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65851faf",
   "metadata": {},
   "source": [
    "### Running a Pipeline\n",
    "\n",
    "To rune a pipeline, you need to fetch the name of pipeline you wanted to try, and then execute with pipeline.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e99e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.fetch(pipeline_name = 'Personalized Email Generator', api_key=vs_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba06cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline.run(\n",
    "    inputs = {\"input_1\": \"https://www.vectorshift.ai/\"},\n",
    "    api_key= vs_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79d88ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_1': \"Hello,\\nWe are XYZ consulting, specializing in crafting growth strategies.\\n\\nOur consulting firm can strategically help VectorShift by streamlining their workflow automation processes, effectively leveraging AI technologies across varied data formats. We can offer customized solutions to automate the creation of marketing copy, personalized outbound emails, and call summaries. We can further assist in enhancing their document, video, and audio file analysis capabilities through advanced AI integration. Additionally, we can guide VectorShift to efficiently use their pre-built templates, improve their application's architecture, and make the transfer of work seamless between no-code and their Python SDK. Our expertise can significantly contribute to enhancing their enterprise solutions, report generation, chatbot volume, outbound emails, RFPs, and proposal generators, ultimately driving the company's growth.\\n\\nAre you available anytime later this week to chat?\\n\\nBest,\\nXYZ\"}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
